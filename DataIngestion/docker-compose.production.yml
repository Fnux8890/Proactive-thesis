# Unified Cloud Configuration for Google Cloud A2 Instance
# Combines production optimizations with parallel processing
# Use with: docker compose -f docker-compose.yml -f docker-compose.cloud-unified.yml up

services:
  # ============================================
  # DATABASE & INFRASTRUCTURE
  # ============================================
  
  # Database - optimized for production
  db:
    environment:
      - POSTGRES_SHARED_BUFFERS=8GB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=24GB
      - POSTGRES_WORK_MEM=256MB
      - POSTGRES_MAINTENANCE_WORK_MEM=2GB
      - POSTGRES_MAX_CONNECTIONS=500
      - POSTGRES_MAX_PARALLEL_WORKERS=24
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 32G
    restart: always

  # Redis for task queue and caching
  redis:
    image: redis:7-alpine
    command: redis-server --maxmemory 8gb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 8G
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # PgBouncer for connection pooling
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    environment:
      - DATABASES_HOST=db
      - DATABASES_PORT=5432
      - DATABASES_DATABASE=postgres
      - DATABASES_USER=postgres
      - DATABASES_PASSWORD=postgres
      - POOL_MODE=transaction
      - MAX_CLIENT_CONN=1000
      - DEFAULT_POOL_SIZE=50
    depends_on:
      db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    restart: always

  # ============================================
  # DATA PROCESSING PIPELINE
  # ============================================

  # Rust Pipeline - production settings
  rust_pipeline:
    environment:
      - RUST_LOG=info
      - BATCH_SIZE=10000
      - NUM_WORKERS=16
    deploy:
      resources:
        limits:
          cpus: '16'
          memory: 32G
    restart: on-failure

  # Preprocessing - comprehensive features
  preprocessing:
    environment:
      - LOG_LEVEL=INFO
      - FEATURE_SET=comprehensive
      - BATCH_SIZE=10000
      - N_WORKERS=8
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
    restart: on-failure

  # Era Detection - optimized
  era_detector:
    environment:
      - RUST_LOG=info
      - CHANNEL_BUFFER_SIZE=10000
      - BATCH_SIZE=5000
      - RAYON_NUM_THREADS=16
    deploy:
      resources:
        limits:
          cpus: '16'
          memory: 32G
    restart: on-failure

  # ============================================
  # PARALLEL FEATURE EXTRACTION
  # ============================================

  # Feature Extraction Coordinator
  feature-coordinator:
    build:
      context: ./feature_extraction
      dockerfile: parallel/coordinator.dockerfile
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@pgbouncer:6432/postgres
      - GPU_WORKERS=4
      - CPU_WORKERS=8
      - GPU_THRESHOLD=500000
      - USE_SMART_DISTRIBUTION=true
      - FEATURE_SET=comprehensive
    depends_on:
      redis:
        condition: service_healthy
      pgbouncer:
        condition: service_started
      preprocessing:
        condition: service_completed_successfully
      era_detector:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 8G
    volumes:
      - ./logs:/app/logs

  # GPU Feature Workers (4x A100)
  feature-gpu-worker-0:
    build:
      context: ./feature_extraction
      dockerfile: parallel/gpu_worker.dockerfile
    environment:
      - WORKER_ID=gpu-0
      - CUDA_VISIBLE_DEVICES=0
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@pgbouncer:6432/postgres
      - FEATURE_SET=comprehensive
    runtime: nvidia
    depends_on:
      - feature-coordinator
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
        limits:
          cpus: '6'
          memory: 80G
    restart: on-failure

  feature-gpu-worker-1:
    extends: feature-gpu-worker-0
    environment:
      - WORKER_ID=gpu-1
      - CUDA_VISIBLE_DEVICES=1
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@pgbouncer:6432/postgres
      - FEATURE_SET=comprehensive
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
        limits:
          cpus: '6'
          memory: 80G

  feature-gpu-worker-2:
    extends: feature-gpu-worker-0
    environment:
      - WORKER_ID=gpu-2
      - CUDA_VISIBLE_DEVICES=2
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@pgbouncer:6432/postgres
      - FEATURE_SET=comprehensive
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]
        limits:
          cpus: '6'
          memory: 80G

  feature-gpu-worker-3:
    extends: feature-gpu-worker-0
    environment:
      - WORKER_ID=gpu-3
      - CUDA_VISIBLE_DEVICES=3
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@pgbouncer:6432/postgres
      - FEATURE_SET=comprehensive
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]
        limits:
          cpus: '6'
          memory: 80G

  # CPU Feature Workers (utilizing remaining vCPUs)
  feature-cpu-worker:
    build:
      context: ./feature_extraction
      dockerfile: parallel/cpu_worker.dockerfile
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@pgbouncer:6432/postgres
      - TSFRESH_N_JOBS=2  # 2 jobs per worker
      - FEATURE_SET=comprehensive
    depends_on:
      - feature-coordinator
    deploy:
      replicas: 6  # 6 workers × 2 CPUs = 12 CPUs
      resources:
        limits:
          cpus: '2'
          memory: 20G
    restart: on-failure

  # ============================================
  # MODEL BUILDING & OPTIMIZATION
  # ============================================

  # Model Builder - uses 2 GPUs after feature extraction
  model_builder:
    environment:
      - DEVICE=gpu
      - EPOCHS=100
      - BATCH_SIZE=256
      - LOG_LEVEL=INFO
      - N_GPUS=2
    runtime: nvidia
    depends_on:
      - feature-coordinator
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
        limits:
          cpus: '8'
          memory: 64G
    restart: on-failure

  # MOEA Optimizer - uses 2 GPUs
  moea_optimizer:
    build:
      context: ./moea_optimizer
      dockerfile: Dockerfile.gpu
    environment:
      - USE_GPU=true
      - N_GPUS=2
      - POPULATION_SIZE=500
      - GENERATIONS=200
    runtime: nvidia
    depends_on:
      - model_builder
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
        limits:
          cpus: '8'
          memory: 32G
    restart: on-failure

  # ============================================
  # MONITORING STACK
  # ============================================

  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    restart: always

  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
    restart: always

  # GPU monitoring
  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    environment:
      - DCGM_EXPORTER_NO_HOSTNAME=1
    ports:
      - "9400:9400"
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
        limits:
          cpus: '0.5'
          memory: 1G
    restart: always

volumes:
  redis-data:
  prometheus-data:
  grafana-data:

# Resource Allocation Summary:
# - Database: 8 CPUs, 32GB RAM
# - Redis + PgBouncer: 4 CPUs, 12GB RAM
# - Rust Pipeline: 16 CPUs, 32GB RAM
# - Preprocessing: 8 CPUs, 16GB RAM
# - Era Detection: 16 CPUs, 32GB RAM
# - Feature Coordinator: 2 CPUs, 8GB RAM
# - GPU Workers: 4×6 = 24 CPUs, 4×80GB = 320GB RAM, 4 GPUs
# - CPU Workers: 6×2 = 12 CPUs, 6×20GB = 120GB RAM
# - Model Builder: 8 CPUs, 64GB RAM, 2 GPUs
# - MOEA Optimizer: 8 CPUs, 32GB RAM, 2 GPUs
# - Monitoring: 3.5 CPUs, 7GB RAM
# Total: ~100 CPUs (oversubscribed 2:1), ~740GB RAM (oversubscribed 2:1), 4 GPUs