FROM rapidsai/base:25.04-cuda12.8-py3.12

# Add conda paths to PATH explicitly
ENV PATH=/opt/conda/bin:/opt/conda/condabin:${PATH}

# Set CUDA environment variables
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install build tools for LightGBM GPU support using conda/mamba
RUN mamba install -y -c conda-forge \
    cmake \
    boost \
    boost-cpp \
    compilers \
    git \
    && mamba clean -afy

# Install Python packages
RUN pip install --no-cache-dir uv \
    && uv pip install --system --no-cache-dir \
    torch==2.3.* pytorch-lightning==2.* \
    pandas==2.* pyarrow==15.* \
    scikit-learn==1.* matplotlib==3.* seaborn==0.13.* \
    typing-extensions joblib==1.4.* \
    mlflow-skinny \
    sqlalchemy==2.* psycopg2-binary

# Build and install LightGBM with CUDA support from source
RUN git clone --recursive --branch stable --depth 1 https://github.com/Microsoft/LightGBM.git /tmp/LightGBM \
    && cd /tmp/LightGBM \
    && mkdir build && cd build \
    && cmake -DUSE_GPU=1 -DUSE_CUDA=1 -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda .. \
    && make -j$(nproc) \
    && cd /tmp/LightGBM/python-package \
    && python setup.py install --precompile \
    && rm -rf /tmp/LightGBM

# Create a writable /tmp/mlflow directory for potential client-side staging
RUN mkdir -p /tmp/mlflow && chmod 777 /tmp/mlflow

ENV PYTHONUNBUFFERED=1
ENV CUDA_LAUNCH_BLOCKING=1
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

WORKDIR /app
COPY src/ ./src/
COPY pyproject.toml ./

# Add a startup script to check GPU availability
RUN echo '#!/bin/bash\necho "Checking GPU availability..."\nnvidia-smi\necho ""\necho "CUDA devices visible to PyTorch:"\npython -c "import torch; print(f\"CUDA available: {torch.cuda.is_available()}\"); print(f\"CUDA device count: {torch.cuda.device_count()}\"); print(f\"Current device: {torch.cuda.current_device() if torch.cuda.is_available() else \"N/A\"}\")"\necho ""\nexec "$@"' > /entrypoint.sh && chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
# Default to training all objectives with GPU
CMD ["python", "-m", "src.training.train_all_objectives"]
