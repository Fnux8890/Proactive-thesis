defmodule IngestionService.Telemetry do
  @moduledoc """
  Telemetry and metrics configuration for the ingestion service.

  This module defines telemetry events, metrics, and handlers for monitoring
  the performance and behavior of the ingestion service.
  """

  use Supervisor
  import Telemetry.Metrics
  require Logger

  @doc """
  Starts the telemetry supervisor.

  ## Parameters

  * `arg` - Argument passed to the supervisor

  ## Returns

  * `{:ok, pid}` - The PID of the started supervisor
  * `{:error, reason}` - If there was an error starting the supervisor
  """
  def start_link(arg) do
    Supervisor.start_link(__MODULE__, arg, name: __MODULE__)
  end

  @impl true
  def init(_arg) do
    children = [
      # Telemetry poller periodically emits measurements
      {:telemetry_poller, measurements: periodic_measurements(), period: 10_000}
    ]

    Supervisor.init(children, strategy: :one_for_one)
  end

  @doc """
  Attaches telemetry event handlers.

  This function attaches handlers to various telemetry events emitted by the ingestion service.

  ## Returns

  * `:ok` - If all handlers were attached successfully
  """
  def attach_handlers do
    # Attach handler for pipeline events
    :telemetry.attach(
      "ingestion-pipeline-events",
      [:ingestion_service, :pipeline, :*],
      &__MODULE__.handle_event/4,
      nil
    )

    # Attach handler for metadata catalog events
    :telemetry.attach(
      "metadata-catalog-events",
      [:ingestion_service, :metadata, :*],
      &__MODULE__.handle_event/4,
      nil
    )

    # Attach handler for database events
    :telemetry.attach(
      "database-queries",
      [:ingestion_service, :repo, :query],
      &__MODULE__.handle_event/4,
      nil
    )

    :ok
  end

  @doc """
  Defines telemetry metrics for the ingestion service.

  This function returns a list of metrics for monitoring the performance and behavior
  of the ingestion service, including pipeline processing, database interactions,
  metadata operations, and system metrics.

  ## Returns

  * List of metrics
  """
  def metrics do
    [
      # Phoenix Metrics
      summary("phoenix.endpoint.stop.duration",
        unit: {:native, :millisecond}
      ),
      summary("phoenix.router_dispatch.stop.duration",
        tags: [:route],
        unit: {:native, :millisecond}
      ),

      # Database Metrics
      summary("ingestion_service.repo.query.total_time",
        unit: {:native, :millisecond},
        description: "The sum of the other measurements"
      ),
      summary("ingestion_service.repo.query.decode_time",
        unit: {:native, :millisecond},
        description: "The time spent decoding the data received from the database"
      ),
      summary("ingestion_service.repo.query.query_time",
        unit: {:native, :millisecond},
        description: "The time spent executing the query"
      ),
      summary("ingestion_service.repo.query.queue_time",
        unit: {:native, :millisecond},
        description: "The time spent waiting for a database connection"
      ),
      summary("ingestion_service.repo.query.idle_time",
        unit: {:native, :millisecond},
        description:
          "The time the connection spent waiting before being checked out for the query"
      ),

      # Pipeline Metrics
      summary("ingestion_service.pipeline.file_watcher.duration",
        unit: {:native, :millisecond},
        description: "File watcher scan duration"
      ),
      counter("ingestion_service.pipeline.file_watcher.files_found",
        description: "Number of files found by the file watcher"
      ),
      summary("ingestion_service.pipeline.producer.duration",
        unit: {:native, :millisecond},
        description: "Producer event generation duration"
      ),
      counter("ingestion_service.pipeline.producer.events_generated",
        description: "Number of events generated by the producer"
      ),
      summary("ingestion_service.pipeline.processor.duration",
        tags: [:processor_type],
        unit: {:native, :millisecond},
        description: "Processor execution duration by type"
      ),
      counter("ingestion_service.pipeline.processor.processed_count",
        tags: [:processor_type],
        description: "Number of items processed by processor type"
      ),
      counter("ingestion_service.pipeline.processor.error_count",
        tags: [:processor_type, :error_type],
        description: "Number of errors by processor and error type"
      ),
      summary("ingestion_service.pipeline.schema_inference.duration",
        unit: {:native, :millisecond},
        description: "Schema inference execution duration"
      ),
      counter("ingestion_service.pipeline.schema_inference.schemas_inferred",
        description: "Number of schemas inferred"
      ),
      summary("ingestion_service.pipeline.data_profiler.duration",
        unit: {:native, :millisecond},
        description: "Data profiler execution duration"
      ),
      summary("ingestion_service.pipeline.data_profiler.quality_score",
        unit: {:native, :percent},
        description: "Average data quality score"
      ),
      summary("ingestion_service.pipeline.time_series_processor.duration",
        unit: {:native, :millisecond},
        description: "Time series processor execution duration"
      ),
      counter("ingestion_service.pipeline.time_series_processor.series_processed",
        description: "Number of time series processed"
      ),
      summary("ingestion_service.pipeline.metadata_enricher.duration",
        unit: {:native, :millisecond},
        description: "Metadata enricher execution duration"
      ),
      counter("ingestion_service.pipeline.metadata_enricher.count",
        description: "Number of events enriched with metadata"
      ),
      summary("ingestion_service.pipeline.validator.duration",
        unit: {:native, :millisecond},
        description: "Validator execution duration"
      ),
      counter("ingestion_service.pipeline.validator.validated_count",
        description: "Number of items validated"
      ),
      counter("ingestion_service.pipeline.validator.validation_errors",
        tags: [:validation_rule],
        description: "Number of validation errors by rule"
      ),
      summary("ingestion_service.pipeline.transformer.duration",
        unit: {:native, :millisecond},
        description: "Transformer execution duration"
      ),
      counter("ingestion_service.pipeline.transformer.transformed_count",
        description: "Number of items transformed"
      ),
      summary("ingestion_service.pipeline.writer.duration",
        unit: {:native, :millisecond},
        description: "Writer execution duration"
      ),
      counter("ingestion_service.pipeline.writer.written_count",
        description: "Number of items written"
      ),
      counter("ingestion_service.pipeline.writer.error_count",
        tags: [:error_type],
        description: "Number of writer errors by type"
      ),

      # Metadata Catalog Metrics
      summary("ingestion_service.metadata.register_dataset.duration",
        unit: {:native, :millisecond},
        description: "Dataset registration duration"
      ),
      counter("ingestion_service.metadata.register_dataset.count",
        tags: [:status],
        description: "Number of datasets registered by status"
      ),
      summary("ingestion_service.metadata.update_dataset.duration",
        unit: {:native, :millisecond},
        description: "Dataset update duration"
      ),
      counter("ingestion_service.metadata.update_dataset.count",
        tags: [:status],
        description: "Number of datasets updated by status"
      ),
      summary("ingestion_service.metadata.get_dataset.duration",
        unit: {:native, :millisecond},
        description: "Dataset retrieval duration"
      ),
      counter("ingestion_service.metadata.get_dataset.count",
        tags: [:status],
        description: "Number of dataset retrievals by status"
      ),
      summary("ingestion_service.metadata.list_datasets.duration",
        unit: {:native, :millisecond},
        description: "Dataset listing duration"
      ),
      summary("ingestion_service.metadata.search_datasets.duration",
        unit: {:native, :millisecond},
        description: "Dataset search duration"
      ),
      summary("ingestion_service.metadata.add_tags.duration",
        unit: {:native, :millisecond},
        description: "Tag addition duration"
      ),
      summary("ingestion_service.metadata.remove_tags.duration",
        unit: {:native, :millisecond},
        description: "Tag removal duration"
      ),
      summary("ingestion_service.metadata.record_lineage.duration",
        unit: {:native, :millisecond},
        description: "Lineage recording duration"
      ),
      summary("ingestion_service.metadata.get_lineage.duration",
        unit: {:native, :millisecond},
        description: "Lineage retrieval duration"
      ),

      # Circuit Breaker Metrics
      counter("ingestion_service.circuit_breaker.trip_count",
        tags: [:service],
        description: "Number of times the circuit breaker has tripped by service"
      ),
      counter("ingestion_service.circuit_breaker.success_count",
        tags: [:service],
        description: "Number of successful operations by service"
      ),
      counter("ingestion_service.circuit_breaker.error_count",
        tags: [:service, :error_type],
        description: "Number of errors by service and type"
      ),

      # VM Metrics
      summary("vm.memory.total",
        unit: {:byte, :kilobyte},
        description: "Total memory allocated by the Erlang VM"
      ),
      summary("vm.total_run_queue_lengths.total",
        description: "Total run queue length for all schedulers"
      ),
      summary("vm.total_run_queue_lengths.cpu",
        description: "Run queue length for CPU-bound processes"
      ),
      summary("vm.total_run_queue_lengths.io",
        description: "Run queue length for IO-bound processes"
      )
    ]
  end

  @doc """
  Defines periodic measurements to be executed by the telemetry poller.

  ## Returns

  * List of measurements
  """
  defp periodic_measurements do
    [
      # VM measurements
      {:process_info, :erlang.system_info(:process_count), :process_count},
      {:process_info, :erlang.memory(:total), :memory_total},
      {:process_info, :erlang.memory(:processes_used), :memory_processes},
      {:process_info, :erlang.memory(:atom_used), :memory_atom},
      {:process_info, :erlang.memory(:binary), :memory_binary},
      {:process_info, :erlang.memory(:ets), :memory_ets},

      # Uptime measurement
      {:ingestion_service_measurements, :uptime, :uptime},

      # Queue sizes and backlogs
      {:ingestion_service_measurements, :get_queue_sizes, :queue_sizes}
    ]
  end

  @doc """
  Custom measurements function for uptime.

  ## Returns

  * Uptime in seconds
  """
  def uptime do
    start_time = Application.get_env(:ingestion_service, :start_time, System.monotonic_time())
    System.convert_time_unit(System.monotonic_time() - start_time, :native, :second)
  end

  @doc """
  Custom measurements function for queue sizes.

  ## Returns

  * Map of queue sizes by stage
  """
  def get_queue_sizes do
    # Get queue sizes from various GenStage processes
    # This would need to be implemented based on the specific GenStage processes in your application
    %{}
  end

  @doc """
  Telemetry event handler function.

  This function is called whenever a telemetry event is emitted. It logs the event
  and stores metrics in Redis for real-time monitoring.

  ## Parameters

  * `event_name` - The name of the event
  * `measurements` - The measurements for the event
  * `metadata` - Metadata for the event
  * `config` - Configuration for the handler
  """
  def handle_event(event_name, measurements, metadata, _config) do
    # Log the event at the appropriate level
    log_level = get_log_level(event_name, metadata)

    Logger.log(log_level, fn ->
      "Telemetry event: #{inspect(event_name)}, " <>
        "measurements: #{inspect(measurements)}, " <>
        "metadata: #{inspect(metadata)}"
    end)

    # Store metrics in Redis for real-time monitoring
    store_metrics_in_redis(event_name, measurements, metadata)

    # Update counters based on event type
    update_counters(event_name, metadata)
  end

  # Determine the appropriate log level based on the event
  defp get_log_level([:ingestion_service, :pipeline, _], metadata) do
    case Map.get(metadata, :status) do
      :error -> :error
      _ -> :debug
    end
  end

  defp get_log_level([:ingestion_service, :metadata, _], metadata) do
    case Map.get(metadata, :status) do
      :error -> :error
      _ -> :debug
    end
  end

  defp get_log_level([:ingestion_service, :circuit_breaker, _], metadata) do
    case Map.get(metadata, :status) do
      :open -> :warning
      :error -> :error
      _ -> :debug
    end
  end

  defp get_log_level(_, _), do: :debug

  # Store metrics in Redis for real-time monitoring
  defp store_metrics_in_redis(event_name, measurements, metadata) do
    # Convert event name to a Redis key
    key = "metrics:" <> Enum.join(event_name, ":")

    # Convert measurements and metadata to JSON
    value =
      Jason.encode!(%{
        timestamp: DateTime.utc_now(),
        measurements: measurements,
        metadata: metadata
      })

    # Store in Redis with expiration (1 hour)
    Redix.command(:redix, ["SETEX", key, 3600, value])

    # Also add to a time-series list (limited to 1000 entries)
    Redix.command(:redix, ["LPUSH", key <> ":history", value])
    Redix.command(:redix, ["LTRIM", key <> ":history", 0, 999])
  rescue
    e ->
      Logger.error("Failed to store metrics in Redis: #{inspect(e)}")
      :ok
  end

  # Update counters based on event type
  defp update_counters([:ingestion_service, :pipeline, stage], metadata) do
    # Increment pipeline stage counter
    counter_key = "counter:pipeline:#{stage}"
    Redix.command(:redix, ["INCR", counter_key])

    # Increment counter by status if available
    if status = Map.get(metadata, :status) do
      Redix.command(:redix, ["INCR", "#{counter_key}:#{status}"])
    end
  rescue
    _ -> :ok
  end

  defp update_counters([:ingestion_service, :metadata, operation], metadata) do
    # Increment metadata operation counter
    counter_key = "counter:metadata:#{operation}"
    Redix.command(:redix, ["INCR", counter_key])

    # Increment counter by status if available
    if status = Map.get(metadata, :status) do
      Redix.command(:redix, ["INCR", "#{counter_key}:#{status}"])
    end
  rescue
    _ -> :ok
  end

  defp update_counters(_, _), do: :ok
end
