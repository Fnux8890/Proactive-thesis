# Combined Docker Compose for DataIngestion (Rust + Python Pipelines)
# Located in DataIngestion/docker-compose.yml

services:
  # --- Rust Pipeline Services ---
  rust_pipeline:
    build:
      context: ./rust_pipeline # Adjusted build context
      dockerfile: Dockerfile
      args:
        CACHEBUST: ${CACHEBUST:-1}
    volumes:
      # Mount the data directory relative to DataIngestion parent
      - ../Data:/app/data:ro
      - ./rust_pipeline/pipeline_logs:/app/logs:rw # Adjusted path
    depends_on:
      redis:
        condition: service_started
      db:
        condition: service_healthy
    environment:
      DATA_SOURCE_PATH: /app/data
      REDIS_URL: redis://redis:6379
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
    networks:
      - ingestion-net

  redis:
    image: redis:alpine
    restart: always
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis-data:/data
    networks:
      - ingestion-net
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # Single Database Service (from rust_pipeline definition)
  db:
    image: timescale/timescaledb:latest-pg16
    restart: always
    environment:
      POSTGRES_USER: "postgres"
      POSTGRES_PASSWORD: "postgres"
    ports:
      - "5432:5432"
    volumes:
      # Mount main init script first (for sensor_data table)
      - ./rust_pipeline/db_init/init.sql:/docker-entrypoint-initdb.d/00_init_sensor_data.sql:ro # Adjusted path
      # Mount script to create extra DBs needed by Python pipeline services
      - ./simulation_data_prep/create_extra_dbs.sql:/docker-entrypoint-initdb.d/01_create_extra_dbs.sql:ro # Adjusted path
      # Volume for pre_process SQL script (if not already handled by preprocess_feat service CMD)
      - ./feature_extraction/pre_process/create_preprocessed_hypertable.sql:/docker-entrypoint-initdb.d/02_create_preprocessed_hypertable.sql:ro
      - ./feature_extraction/pre_process/phenotype_ingest.py:/app/phenotype_ingest.py:ro # Make accessible if needed by init
      - ./feature_extraction/pre_process/phenotype.json:/app/phenotype.json:ro
      - ./feature_extraction/pre_process/db_utils.py:/app/db_utils.py:ro
      - postgres-data:/var/lib/postgresql/data
    networks:
      - ingestion-net
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres -d postgres || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dpage/pgadmin4:latest
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: "admin@example.com"
      PGADMIN_DEFAULT_PASSWORD: "admin"
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_SETUP_EMAIL: "admin@example.com"
      PGADMIN_SETUP_PASSWORD: "admin"
    volumes:
      - pgadmin-data:/var/lib/pgadmin
      - ./rust_pipeline/servers.json:/pgadmin4/servers.json:ro # Adjusted path
    ports:
      - "5050:80"
    depends_on:
      db:
        condition: service_healthy
    networks:
      - ingestion-net


  era_detector:
    build:
      context: ./feature_extraction/era_detection_rust
      dockerfile: dockerfile # Corrected case
      args:
        CACHEBUST: ${CACHEBUST:-2} # Optional: if your Dockerfile uses it
    container_name: era_detector # ADDED
    # entrypoint: /bin/bash # Reverted: Use Dockerfile's ENTRYPOINT
    command: [
      "--input-parquet", "/app/data/shared_processed/MegaEra_All_Data_processed_segment_1.parquet",
      "--output-dir", "/app/data/shared_processed/era_labels",
      "--output-suffix", "_eras",
      "--db-dsn", "postgresql://postgres:postgress@db:5432/postgres",
      "--signal-cols", "dli_sum,co2_status,light_intensity_lux,co2_measured_ppm,co2_required_ppm,radiation_w_m2,sun_radiation_forecast_w_m2,curtain_2_percent,vent_pos_1_percent,curtain_1_percent,vent_pos_2_percent",
      "--resample-every", "5m",
      "--pelt-min-size", "48",
      "--bocpd-lambda", "200.0",
      "--hmm-states", "5"
    ]
    environment:
      RUST_LOG: debug # Set to debug for more verbose logs; can be info, warn, error
      DB_DSN: postgresql://postgres:postgres@db:5432/postgres # Ensure this matches your TimescaleDB setup
      # Add any other environment variables your Rust application might require
    volumes: # MODIFIED
      # Mount for Parquet input override
      - ./feature_extraction/data/processed:/app/data/shared_processed:rw
      # Optional: Mount for logs written to file by Rust app, if configured
      # - ./feature_extraction/era_detection_rust/era_detector_logs:/app/logs:rw
    depends_on:
      db:
        condition: service_healthy # Ensures TimescaleDB is ready
      raw_prep_lite:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]
    networks:
      - ingestion-net

  feature_extractor:
    build:
      context: ./feature_extraction/feature
      dockerfile: feature.dockerfile
    container_name: feature_extractor
    environment:
      PYTHONUNBUFFERED: "1"
      DB_DSN: postgresql://postgres:postgres@db:5432/postgres
      ERA_DEFINITIONS_PATH_ENV: /app/data/input_eras/era_labels 
      ERA_ID_COLUMN_KEY_ENV: era_level_B 
      USE_GPU: ${USE_GPU:-true}
    volumes:
      # Mount for era definition files (output from era_detector)
      - ./feature_extraction/data/processed:/app/data/input_eras:ro
      # Mount for the script to write its output features
      - ./feature_extraction/data/features:/app/data/output_features:rw
      # Mount the source code for easier development
      - ./feature_extraction/feature:/app:rw
    depends_on:
      db:
        condition: service_healthy
      era_detector: 
        condition: service_completed_successfully
    networks:
      - ingestion-net
    deploy: 
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]

  # --- Python Pipeline Services ---
  # data-prep:
  #   build:
  #     context: ./simulation_data_prep # Adjusted build context
  #     dockerfile: Dockerfile
  #   env_file:
  #     - ./.env # Load from the same directory as this docker-compose.yml
  #   command: [ "prefect", "worker", "start", "--pool", "default-process-pool", "--limit", "1" ] # Use direct CLI command
  #   environment:
  #     - PREFECT_API_URL=http://orion:4200/api
  #     - PREFECT_LOGGING_LEVEL=DEBUG
  #     - PYTHONUNBUFFERED=1 # Ensure logs appear immediately
  #     - PYTHONPATH=/app # Add project root to Python path
  #     - SKIP_GE_VALIDATION=true # Skip Great Expectations validation
  #     # Pass through host environment variables
  #     # - USE_GPU=${USE_GPU:-false} # Example, passed from host
  #     # - PARALLEL_EXECUTION=${PARALLEL_EXECUTION:-true} # Example
  #     # - ONLY_EARLIEST_DATE=${ONLY_EARLIEST_DATE:-false} # Example
  #   depends_on:
  #     orion:
  #       condition: service_healthy
  #     db:
  #       condition: service_healthy
  #   volumes:
  #     - ./simulation_data_prep/src:/app/src
  #     - ./simulation_data_prep/output:/app/output
  #     - ./simulation_data_prep/plant_config.json:/app/plant_config.json
  #     - ./simulation_data_prep/feast_repo:/app/feast_repo
  #     - ./simulation_data_prep/great_expectations:/app/great_expectations
  #     - ./simulation_data_prep/dao:/app/dao
  #     - ./simulation_data_prep/transforms:/app/transforms
  #     - ./simulation_data_prep/loading:/app/loading
  #     - ./simulation_data_prep/validation:/app/validation
  #     - prefect_data:/root/.prefect
  #   networks:
  #     - ingestion-net

  # orion:
  #   build:
  #     context: ./simulation_data_prep # Adjusted build context
  #     dockerfile: orion.Dockerfile # Relative path from context
  #   command: prefect server start --host 0.0.0.0 --log-level debug
  #   # Map host port 4201 to container port 4200 for UI access
  #   ports:
  #     - "4201:4200"
  #   environment:
  #     - PREFECT_LOGGING_LEVEL=DEBUG
  #     # Point the UI (browser) to the host port where the API is exposed
  #     - PREFECT_UI_API_URL=http://localhost:4201/api
  #     - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:postgres@db:5432/prefect
  #   volumes:
  #     - prefect_db:/root/.prefect/
  #   depends_on:
  #     db:
  #       condition: service_healthy
  #   healthcheck:
  #     test: [ "CMD-SHELL", "curl -fs http://localhost:4200/health/live || exit 1" ]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   networks:
  #     - ingestion-net

  # mlflow-server:
  #   build:
  #     context: ./simulation_data_prep # Adjusted build context
  #     dockerfile: mlflow.Dockerfile # Relative path from context
  #   # Reverted command to use default filesystem artifact root
  #   command: >
  #     mlflow server --host 0.0.0.0 --port 5000  --backend-store-uri postgresql+psycopg2://postgres:postgres@db:5432/mlflow  --default-artifact-root /mlflow/artifacts 
  #   environment:
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=postgres
  #   # Re-enabled volume mount for artifacts
  #   volumes:
  #     - mlflow_data:/mlflow/artifacts:rw
  #   ports:
  #     - "5001:5000"
  #   depends_on:
  #     db:
  #       condition: service_healthy
  #   networks:
  #     - ingestion-net

  # # Test service for Python pipeline unit tests
  # test:
  #   build:
  #     context: ./simulation_data_prep # Adjusted build context
  #     dockerfile: Dockerfile
  #   working_dir: /app # Set working directory for consistency
  #   env_file:
  #     - ./.env # Load from the same directory as this docker-compose.yml
  #   # Add PYTHONPATH to help with module discovery
  #   environment:
  #     - PYTHONPATH=/app
  #   depends_on:
  #     db:
  #       condition: service_healthy
  #   entrypoint: [ "pytest", "--maxfail=1" ]
  #   volumes:
  #     # Mount source code needed for tests (read-only is fine for code)
  #     - ./simulation_data_prep/src:/app/src:ro
  #     - ./simulation_data_prep/dao:/app/dao:ro
  #     - ./simulation_data_prep/transforms:/app/transforms:ro
  #     - ./simulation_data_prep/loading:/app/loading:ro
  #     - ./simulation_data_prep/validation:/app/validation:ro
  #     # Mount the tests directory read-write to allow log file creation
  #     - ./simulation_data_prep/src/tests:/app/src/tests:rw
  #     # Mount config file needed by tests
  #     - ./simulation_data_prep/plant_config.json:/app/plant_config.json:ro
  #     # Mount sample data needed by tests (relative to docker-compose.yml location)
  #     - ../Data:/app/data:ro
  #   networks:
  #     - ingestion-net

  # # --- New one-shot service for Prefect Deployment ---
  # prefect-deployer:
  #   build:
  #     context: ./simulation_data_prep
  #     dockerfile: Dockerfile
  #   restart: "no"
  #   networks:
  #     - ingestion-net
  #   env_file:
  #     - ./.env
  #   environment:
  #     - PREFECT_API_URL=http://orion:4200/api
  #   # Restore original command 
  #   command: [ "echo", "Deployer container ready, awaiting command..." ] # Was: ["sleep", "infinity"]
  #   volumes:
  #     # Mount the entire simulation_data_prep directory
  #     - ./simulation_data_prep:/app
  #   working_dir: /app
  #   depends_on:
  #     orion:
  #       condition: service_healthy

  model_builder:
    build:
      context: ./model_builder # NEW â†’ use the Dockerfile you just wrote
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility # Ensure capabilities are set here if needed by runtime
      - GPU_TAG=gtx1660super # Example: Set the tag for the specific GPU being used
      - MLFLOW_TRACKING_URI=http://mlflow-server:5000 # Explicitly set URI
    volumes:
      - ./simulation_data_prep/output:/data:ro
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: [ "python", "-m", "src.training.train_surrogate", "--epochs", "50", "--data-dir", "/data", "--model-dir", "/models" ]
    depends_on:
      # mlflow-server:
      #   condition: service_started # Wait for mlflow server (adjust if healthcheck added)
      db:
        # MLflow server itself depends on DB being healthy
        condition: service_healthy
    # Add the service to the user-defined network
    networks:
      - ingestion-net

  # --- Feature Extraction Pipeline Services (Modified and New) ---
  # Stage A: Raw-Prep Lite (Uses preprocess.py with specific config)
  raw_prep_lite:
    build:
      context: ./feature_extraction/pre_process
      dockerfile: preprocess.dockerfile
    container_name: raw_prep_lite
    volumes:
      - ./feature_extraction/data/input:/app/data/input:ro
      - ./feature_extraction/data/processed:/app/data/output:rw
      - ./feature_extraction/data_processing_config.json:/app/config/data_processing_config.json:ro
    # preprocess.py now handles calling other ingestion scripts internally
    command: uv run python preprocess.py
    environment:
      - SKIP_ERA_FEATURE=true
      - PROCESS_ERA_IDENTIFIER=MegaEra_All_Data
      - APP_CONFIG_PATH=/app/config/data_processing_config.json
      - APP_OUTPUT_DIR=/app/data/output
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_HOST=db
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-postgres}
    depends_on:
      db:
        condition: service_healthy
      rust_pipeline:
        condition: service_completed_successfully
    networks:
      - ingestion-net
    deploy: 
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]

  # Stage B: Dynamic Era Discovery
  # The era_detector service definition previously here has been consolidated
  # with the one defined earlier in this file (around line 88).
  # The consolidated service defaults to database input but can be overridden
  # to use Parquet input via `docker-compose run era_detector --input-parquet ...`

  # Stage C: Auto-Era-Config Generator
  # auto_era_gen:
  #     context: ./feature_extraction/era_detection # Dockerfile can be simple, just needs python and pandas
  #     dockerfile: auto_era_gen.dockerfile # NEW Dockerfile for this tiny script
  #   container_name: auto_era_generator
  #   volumes:
  #     # Input is the Level C labels from era_detector, output is the JSON config
  #     - ./feature_extraction/data/processed:/app/data/processed:rw
  #   # Command to run auto_era_config_gen.py
  #   # Input: <output_suffix_from_era_detector>_era_labels_levelC.parquet
  #   # Output: auto_era_definitions.json (placed in /app/data/processed for full_preprocess to access)
  #   command: >
  #     uv run python auto_era_config_gen.py  /app/data/processed/MegaEra_All_Data_Segment1_era_labels_levelC.parquet  /app/data/processed/auto_era_definitions.json 
  #   depends_on:
  #     era_detector:
  #       condition: service_completed_successfully
  #   networks:
  #     - ingestion-net

  # # Stage D: Full Pre-processing Per Era (Manual + Auto)
  # # This service reuses the preprocess.dockerfile but with different ENV settings
  # full_preprocess:
  #   build:
  #     context: ./feature_extraction/pre_process
  #     dockerfile: preprocess.dockerfile
  #   container_name: full_preprocess
  #   volumes:
  #     - ./feature_extraction/data/input:/app/data/input:ro
  #     - ./feature_extraction/data/processed:/app/data/output:rw # Main output for processed era segments
  #     - ./feature_extraction/data_processing_config.json:/app/config/data_processing_config.json:ro
  #     # Mount the auto-generated era config for merging
  #     - ./feature_extraction/data/processed/auto_era_definitions.json:/app/data/processed/auto_era_definitions.json:ro
  #   command: >
  #     sh -c " uv run python phenotype_ingest.py && \ uv run python fetch_external_weather.py && \ uv run python fetch_energy.py && \ uv run python preprocess.py"
  #   environment:
  #     - SKIP_ERA_FEATURE=false # Ensure EraFeatureGenerator runs in this stage
  #     - EXTRA_ERA_CONFIG=/app/data/processed/auto_era_definitions.json # Path to auto-generated eras
  #     - APP_CONFIG_PATH=/app/config/data_processing_config.json
  #     - APP_OUTPUT_DIR=/app/data/output
  #     - DB_USER=${DB_USER:-postgres}
  #     - DB_PASSWORD=${DB_PASSWORD:-postgres}
  #     - DB_HOST=db
  #     - DB_PORT=${DB_PORT:-5432}
  #     - DB_NAME=${DB_NAME:-postgres}
  #     # PROCESS_ERA_IDENTIFIER is NOT set here, so preprocess.py processes all eras found in merged config
  #   depends_on:
  #     auto_era_gen:
  #       condition: service_completed_successfully
  #     db:
  #       condition: service_healthy # preprocess.py interacts with DB
  #   networks:
  #     - ingestion-net

  # Stage E: Feature Extraction (Modified dependencies)
  # (Existing preprocess_feat service might be replaced by raw_prep_lite + full_preprocess)
  # The original preprocess_feat is more like Stage D if PROCESS_ERA_IDENTIFIER was set for a specific era.
  # We will keep feature_extraction, feature_extraction_gpu, feature_analysis and adjust their dependencies.

  # Delete or comment out original preprocess_feat if its role is fully covered by raw_prep_lite & full_preprocess
  # preprocess_feat:
  #   ...

  feature_extraction:
    # Existing, but depends on full_preprocess now
    build:
      context: ./feature_extraction/feature
      dockerfile: feature.dockerfile
    container_name: feature_extraction
    volumes:
      - ./feature_extraction/data/processed:/app/data/processed:ro # Input from full_preprocess
      - ./feature_extraction/data/output:/app/data/output:rw # Output for feature_extraction service itself
      - ./feature_extraction/config:/app/config:ro
    command: uv run python extract_features.py
    environment:
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_HOST=db
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-postgres}
    depends_on:
      era_detector:
        condition: service_completed_successfully
      db:
        # If extract_features.py also connects to DB
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices: [ { capabilities: [ gpu ], driver: nvidia } ]
    networks:
      - ingestion-net

  # feature_extraction_gpu:
  #   # Existing, but depends on full_preprocess
  #   build:
  #     context: ./feature_extraction/feature-gpu
  #     dockerfile: feature_gpu.dockerfile
  #   deploy:
  #     resources:
  #       reservations:
  #         devices: [ { capabilities: [ gpu ], driver: nvidia } ]
  #   container_name: feature_extraction_gpu
  #   volumes:
  #     - ./feature_extraction/data/processed:/app/data/processed:ro
  #     - ./feature_extraction/data/output:/app/data/output:rw
  #   environment:
  #     - DB_USER=${DB_USER:-postgres}
  #     - DB_PASSWORD=${DB_PASSWORD:-postgres}
  #     - DB_HOST=db
  #     - DB_PORT=${DB_PORT:-5432}
  #     - DB_NAME=${DB_NAME:-postgres}
  #   depends_on:
  #     era_detector:
  #       condition: service_completed_successfully
  #     db:
  #       condition: service_healthy
  #   networks:
  #     - ingestion-net

  # feature_analysis:
  #   # Existing, could depend on rust_pipeline or full_preprocess output
  #   build:
  #     context: ./feature_extraction # Changed context to the parent directory
  #     dockerfile: ./data_analysis/analysis.dockerfile # Path to Dockerfile from new context
  #   volumes:
  #     - ./feature_extraction/data_processing_config.json:/app/input_config/data_processing_config.json:ro
  #     - ./feature_extraction/data/analysis:/app/output_data:rw
  #   depends_on:
  #     # Choose dependency: rust_pipeline for raw analysis, or full_preprocess if it needs cleaned data
  #     feature_extraction:
  #       condition: service_completed_successfully
  #     db:
  #       condition: service_healthy
  #   environment:
  #     - DB_HOST=db
  #     - DB_USER=${DB_USER:-postgres}
  #     - DB_PASSWORD=${DB_PASSWORD:-postgres}
  #     - DB_NAME=${DB_NAME:-postgres}
  #     - PYTHONUNBUFFERED=1
  #   networks:
  #     - ingestion-net

# --- Volumes ---
volumes:
  postgres-data:
  redis-data:
  pgadmin-data:
  prefect_data:
  prefect_db:
  mlflow_data:
  model_builder_mlflow_staging: {} # --- Network ---
networks:
  ingestion-net:
    name: ingestion-net
    driver: bridge
