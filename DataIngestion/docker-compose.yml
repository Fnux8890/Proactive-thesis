# Data Ingestion & Optimization Pipeline
# Pipeline Flow: Data Ingestion → Preprocessing → Era Detection → Feature Extraction → Model Building → MOEA Optimization
# All GPU-enabled services are properly configured with NVIDIA runtime
# 
# Before running: Copy .env.example to .env and adjust values as needed

services:
  # ============================================
  # INFRASTRUCTURE SERVICES
  # ============================================
  
  # TimescaleDB - Main database for all pipeline stages
  db:
    image: timescale/timescaledb:latest-pg16
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      # Initialize database schemas
      - ./rust_pipeline/db_init/init.sql:/docker-entrypoint-initdb.d/00_init_sensor_data.sql:ro
      - ./simulation_data_prep/create_extra_dbs.sql:/docker-entrypoint-initdb.d/01_create_extra_dbs.sql:ro
      - ./feature_extraction/pre_process/create_preprocessed_hypertable.sql:/docker-entrypoint-initdb.d/02_create_preprocessed_hypertable.sql:ro
      - postgres-data:/var/lib/postgresql/data
    networks:
      - pipeline-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis - For caching and job queuing
  redis:
    image: redis:alpine
    restart: always
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis-data:/data
    networks:
      - pipeline-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # pgAdmin - Database administration
  pgadmin:
    image: dpage/pgadmin4:latest
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_SETUP_EMAIL: "admin@example.com"
      PGADMIN_SETUP_PASSWORD: "admin"
    volumes:
      - pgadmin-data:/var/lib/pgadmin
      - ./rust_pipeline/servers.json:/pgadmin4/servers.json:ro
    ports:
      - "5050:80"
    depends_on:
      db:
        condition: service_healthy
    networks:
      - pipeline-net

  # MLflow - Experiment tracking (optional, uncomment if needed)
  # mlflow-server:
  #   build:
  #     context: ./simulation_data_prep
  #     dockerfile: mlflow.Dockerfile
  #   command: >
  #     mlflow server 
  #     --host 0.0.0.0 
  #     --port 5000 
  #     --backend-store-uri postgresql+psycopg2://postgres:postgres@db:5432/mlflow 
  #     --default-artifact-root /mlflow/artifacts
  #   environment:
  #     POSTGRES_USER: postgres
  #     POSTGRES_PASSWORD: postgres
  #   volumes:
  #     - mlflow_data:/mlflow/artifacts:rw
  #   ports:
  #     - "5000:5000"
  #   depends_on:
  #     db:
  #       condition: service_healthy
  #   networks:
  #     - pipeline-net

  # ============================================
  # STAGE 1: DATA INGESTION (Rust-based)
  # ============================================
  
  rust_pipeline:
    build:
      context: ./rust_pipeline
      dockerfile: Dockerfile
      args:
        CACHEBUST: ${CACHEBUST:-1}
    container_name: rust_data_ingestion
    volumes:
      - ../Data:/app/data:ro
      - ./rust_pipeline/pipeline_logs:/app/logs:rw
    depends_on:
      redis:
        condition: service_started
      db:
        condition: service_healthy
    environment:
      DATA_SOURCE_PATH: /app/data
      REDIS_URL: redis://redis:6379
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
    networks:
      - pipeline-net

  # ============================================
  # STAGE 2-4: SPARSE PIPELINE (Integrated Processing)
  # Handles preprocessing, era detection, and feature extraction
  # in one GPU-accelerated container for sparse data (>90% missing)
  # ============================================
  
  sparse_pipeline:
    build:
      context: ./gpu_feature_extraction
      dockerfile: Dockerfile
    container_name: sparse_gpu_pipeline
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
      RUST_LOG: ${RUST_LOG:-info}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
      # Sparse pipeline specific settings
      SPARSE_MODE: "true"
      DISABLE_GPU: ${DISABLE_GPU:-false}
    command: [
      "--sparse-mode",
      "--start-date", "${START_DATE:-2014-01-01}",
      "--end-date", "${END_DATE:-2014-12-31}",
      "--batch-size", "${BATCH_SIZE:-24}",  # Window size in hours
      "--min-era-rows", "10"  # Minimal for sparse data
    ]
    volumes:
      - ./gpu_feature_extraction/checkpoints:/tmp/gpu_sparse_pipeline:rw
    depends_on:
      db:
        condition: service_healthy
      rust_pipeline:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # LEGACY SERVICES - Kept for reference but not used with sparse data
  # ============================================
  
  # Feature extraction for Level A (PELT) - Large eras
  feature_extraction_level_a:
    build:
      context: ./feature_extraction
      dockerfile: feature/feature.dockerfile
    container_name: feature_extraction_level_a
    environment:
      PYTHONUNBUFFERED: "1"
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      FEATURES_TABLE: tsfresh_features_level_a
      USE_GPU: ${USE_GPU:-true}
      FEATURE_SET: ${FEATURE_SET_LEVEL_A:-minimal}
      BATCH_SIZE: ${FEATURE_BATCH_SIZE_A:-10}
      ERA_LEVEL: A
      MIN_ERA_ROWS: ${MIN_ERA_ROWS_A:-1000}
      N_JOBS: ${N_JOBS:--1}
    volumes:
      - ./feature_extraction/data/features:/app/data/output_features:rw
    command: python -m feature.extract_features_enhanced
    depends_on:
      db:
        condition: service_healthy
      era_detector:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for feature extraction
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Feature extraction for Level B (BOCPD) - Medium eras
  feature_extraction_level_b:
    build:
      context: ./feature_extraction
      dockerfile: feature/feature.dockerfile
    container_name: feature_extraction_level_b
    environment:
      PYTHONUNBUFFERED: "1"
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      FEATURES_TABLE: tsfresh_features_level_b
      USE_GPU: ${USE_GPU:-true}
      FEATURE_SET: ${FEATURE_SET_LEVEL_B:-minimal}
      BATCH_SIZE: ${FEATURE_BATCH_SIZE_B:-20}
      ERA_LEVEL: B
      MIN_ERA_ROWS: ${MIN_ERA_ROWS_B:-500}
      N_JOBS: ${N_JOBS:--1}
    volumes:
      - ./feature_extraction/data/features:/app/data/output_features:rw
    command: python -m feature.extract_features_enhanced
    depends_on:
      db:
        condition: service_healthy
      era_detector:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for feature extraction
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Feature extraction for Level C (HMM) - Small eras
  feature_extraction_level_c:
    build:
      context: ./feature_extraction
      dockerfile: feature/feature.dockerfile
    container_name: feature_extraction_level_c
    environment:
      PYTHONUNBUFFERED: "1"
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      FEATURES_TABLE: tsfresh_features_level_c
      USE_GPU: ${USE_GPU:-true}
      FEATURE_SET: ${FEATURE_SET_LEVEL_C:-minimal}
      BATCH_SIZE: ${FEATURE_BATCH_SIZE_C:-100}
      ERA_LEVEL: C
      MIN_ERA_ROWS: ${MIN_ERA_ROWS_C:-50}
      N_JOBS: ${N_JOBS:--1}
    volumes:
      - ./feature_extraction/data/features:/app/data/output_features:rw
    command: python -m feature.extract_features_enhanced
    depends_on:
      db:
        condition: service_healthy
      era_detector:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for feature extraction
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Keep the original as an alias for backward compatibility
  feature_extraction:
    extends:
      service: feature_extraction_level_b

  # ============================================
  # STAGE 4.5: GPU-ACCELERATED FEATURE EXTRACTION (Alternative to CPU)
  # ============================================
  
  # GPU Feature extraction for Level A - Large structural eras
  gpu_feature_extraction_level_a:
    build:
      context: ./gpu_feature_extraction
      dockerfile: Dockerfile
    container_name: gpu_feature_extraction_level_a
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
      RUST_LOG: ${RUST_LOG:-info}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
      GPU_BATCH_SIZE: ${GPU_BATCH_SIZE_A:-500}
    command: [
      "--era-level", "A",
      "--min-era-rows", "${MIN_ERA_ROWS_A:-1000}",
      "--batch-size", "${BATCH_SIZE_A:-500}",
      "--features-table", "gpu_features_level_a"
    ]
    depends_on:
      db:
        condition: service_healthy
      era_detector:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # GPU Feature extraction for Level B - Medium operational eras  
  gpu_feature_extraction_level_b:
    build:
      context: ./gpu_feature_extraction
      dockerfile: Dockerfile
    container_name: gpu_feature_extraction_level_b
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
      RUST_LOG: ${RUST_LOG:-info}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
      GPU_BATCH_SIZE: ${GPU_BATCH_SIZE_B:-1000}
    command: [
      "--era-level", "B",
      "--min-era-rows", "${MIN_ERA_ROWS_B:-500}",
      "--batch-size", "${BATCH_SIZE_B:-1000}",
      "--features-table", "gpu_features_level_b"
    ]
    depends_on:
      db:
        condition: service_healthy
      era_detector:
        condition: service_completed_successfully
      # Sequential execution: wait for Level A to complete
      gpu_feature_extraction_level_a:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # GPU Feature extraction for Level C - Small detailed eras
  gpu_feature_extraction_level_c:
    build:
      context: ./gpu_feature_extraction
      dockerfile: Dockerfile
    container_name: gpu_feature_extraction_level_c
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
      RUST_LOG: ${RUST_LOG:-info}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
      GPU_BATCH_SIZE: ${GPU_BATCH_SIZE_C:-100}
    command: [
      "--era-level", "C",
      "--min-era-rows", "${MIN_ERA_ROWS_C:-5000}",
      "--batch-size", "${BATCH_SIZE_C:-100}",
      "--features-table", "gpu_features_level_c"
    ]
    depends_on:
      db:
        condition: service_healthy
      era_detector:
        condition: service_completed_successfully
      # Sequential execution: wait for Level B to complete
      gpu_feature_extraction_level_b:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Keep the original as an alias for backward compatibility
  gpu_feature_extraction:
    extends:
      service: gpu_feature_extraction_level_b

  # ============================================
  # STAGE 5: MODEL BUILDING (Multiple objectives)
  # ============================================
  
  # Train all MOEA objective models
  model_builder:
    build:
      context: ./model_builder
      dockerfile: dockerfile
    container_name: model_builder_all_objectives
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
      GPU_TAG: ${GPU_TAG:-default_gpu}
      # Use sparse pipeline features
      USE_SPARSE_FEATURES: ${USE_SPARSE_FEATURES:-true}
      FEATURE_TABLES: "sparse_features"  # Table created by sparse pipeline
    volumes:
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: ["python", "-m", "src.training.train_all_objectives"]
    depends_on:
      db:
        condition: service_healthy
      # Wait for sparse pipeline to complete
      sparse_pipeline:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for model training
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # STAGE 6: MOEA OPTIMIZATION (CPU/GPU)
  # ============================================
  
  moea_optimizer_cpu:
    build:
      context: ./moea_optimizer
      dockerfile: Dockerfile
    container_name: moea_optimizer_cpu
    environment:
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
      CONFIG_PATH: /app/config/moea_config_cpu.toml
      DEVICE: cpu
    volumes:
      - ./moea_optimizer/config:/app/config:ro
      - ./model_builder/models:/app/models:ro
      - ./moea_optimizer/results:/app/results:rw
    command: ["python", "-m", "src.cli", "run", "--config", "/app/config/moea_config_cpu.toml"]
    depends_on:
      db:
        condition: service_healthy
      model_builder:
        condition: service_completed_successfully
    networks:
      - pipeline-net

  moea_optimizer_gpu:
    build:
      context: ./moea_optimizer
      dockerfile: Dockerfile.gpu
    container_name: moea_optimizer_gpu
    # IPC mode for shared memory
    ipc: host
    # Ulimits for memory and stack
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    # Shared memory size (optional, but good to set explicitly)
    # With 32GB system RAM, 8GB shared memory is reasonable
    shm_size: '8gb'
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
      CONFIG_PATH: /app/config/moea_config_gpu.toml
      DEVICE: cuda
      CUDA_DEVICE_ID: 0
    volumes:
      - ./moea_optimizer/config:/app/config:ro
      - ./model_builder/models:/app/models:ro
      - ./moea_optimizer/results:/app/results:rw
    command: ["python", "-m", "src.cli", "run", "--config", "/app/config/moea_config_gpu.toml"]
    depends_on:
      db:
        condition: service_healthy
      model_builder:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for MOEA optimization
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # UTILITY: Run specific pipeline stages
  # ============================================
  
  # Alternative: Train only LSTM surrogate model
  model_builder_lstm:
    build:
      context: ./model_builder
      dockerfile: dockerfile
    container_name: model_builder_lstm
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      GPU_TAG: ${GPU_TAG:-default_gpu}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
    volumes:
      - ./simulation_data_prep/output:/data:ro
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: ["python", "-m", "src.training.train_surrogate", "--epochs", "50", "--data-dir", "/data", "--model-dir", "/models"]
    depends_on:
      db:
        condition: service_healthy
    networks:
      - pipeline-net
    profiles: ["lstm"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Alternative: Train only LightGBM model for single objective
  model_builder_lightgbm:
    build:
      context: ./model_builder
      dockerfile: dockerfile
    container_name: model_builder_lightgbm
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
    volumes:
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: ["python", "-m", "src.training.train_lightgbm_surrogate", "--target", "energy_consumption", "--gpu"]
    depends_on:
      db:
        condition: service_healthy
      feature_extraction:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    profiles: ["lightgbm"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# ============================================
# VOLUMES
# ============================================
volumes:
  postgres-data:
  redis-data:
  pgadmin-data:
  mlflow_data:

# ============================================
# NETWORKS
# ============================================
networks:
  pipeline-net:
    name: greenhouse-pipeline
    driver: bridge