# Data Ingestion & Optimization Pipeline
# Pipeline Flow: Data Ingestion → Preprocessing → Era Detection → Feature Extraction → Model Building → MOEA Optimization
# All GPU-enabled services are properly configured with NVIDIA runtime
# 
# Before running: Copy .env.example to .env and adjust values as needed

services:
  # ============================================
  # INFRASTRUCTURE SERVICES
  # ============================================
  
  # TimescaleDB - Main database for all pipeline stages
  db:
    image: timescale/timescaledb:latest-pg16
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      # Initialize database schemas
      - ./rust_pipeline/db_init/init.sql:/docker-entrypoint-initdb.d/00_init_sensor_data.sql:ro
      - ./simulation_data_prep/create_extra_dbs.sql:/docker-entrypoint-initdb.d/01_create_extra_dbs.sql:ro
      - ./feature_extraction/pre_process/create_preprocessed_hypertable.sql:/docker-entrypoint-initdb.d/02_create_preprocessed_hypertable.sql:ro
      - postgres-data:/var/lib/postgresql/data
    networks:
      - pipeline-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis - For caching and job queuing
  redis:
    image: redis:alpine
    restart: always
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis-data:/data
    networks:
      - pipeline-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # pgAdmin - Database administration
  pgadmin:
    image: dpage/pgadmin4:latest
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_SETUP_EMAIL: "admin@example.com"
      PGADMIN_SETUP_PASSWORD: "admin"
    volumes:
      - pgadmin-data:/var/lib/pgadmin
      - ./rust_pipeline/servers.json:/pgadmin4/servers.json:ro
    ports:
      - "5050:80"
    depends_on:
      db:
        condition: service_healthy
    networks:
      - pipeline-net

  # MLflow - Experiment tracking (optional, uncomment if needed)
  # mlflow-server:
  #   build:
  #     context: ./simulation_data_prep
  #     dockerfile: mlflow.Dockerfile
  #   command: >
  #     mlflow server 
  #     --host 0.0.0.0 
  #     --port 5000 
  #     --backend-store-uri postgresql+psycopg2://postgres:postgres@db:5432/mlflow 
  #     --default-artifact-root /mlflow/artifacts
  #   environment:
  #     POSTGRES_USER: postgres
  #     POSTGRES_PASSWORD: postgres
  #   volumes:
  #     - mlflow_data:/mlflow/artifacts:rw
  #   ports:
  #     - "5000:5000"
  #   depends_on:
  #     db:
  #       condition: service_healthy
  #   networks:
  #     - pipeline-net

  # ============================================
  # STAGE 1: DATA INGESTION (Rust-based)
  # ============================================
  
  rust_pipeline:
    build:
      context: ./rust_pipeline
      dockerfile: Dockerfile
      args:
        CACHEBUST: ${CACHEBUST:-1}
    container_name: rust_data_ingestion
    volumes:
      - ../Data:/app/data:ro
      - ./rust_pipeline/pipeline_logs:/app/logs:rw
    depends_on:
      redis:
        condition: service_started
      db:
        condition: service_healthy
    environment:
      DATA_SOURCE_PATH: /app/data
      REDIS_URL: redis://redis:6379
      DATABASE_URL: postgresql://postgres:postgres@db:5432/postgres
    networks:
      - pipeline-net

  # ============================================
  # STAGE 2: DATA PREPROCESSING
  # ============================================
  
  preprocessing:
    build:
      context: ./feature_extraction/pre_process
      dockerfile: preprocess.dockerfile
    container_name: data_preprocessing
    volumes:
      - ./feature_extraction/data/input:/app/data/input:ro
      - ./feature_extraction/data/processed:/app/data/output:rw
      - ./feature_extraction/data_processing_config.json:/app/config/data_processing_config.json:ro
    command: uv run python preprocess.py
    environment:
      SKIP_ERA_FEATURE: "true"
      PROCESS_ERA_IDENTIFIER: MegaEra_All_Data
      APP_CONFIG_PATH: /app/config/data_processing_config.json
      APP_OUTPUT_DIR: /app/data/output
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
    depends_on:
      db:
        condition: service_healthy
      rust_pipeline:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for preprocessing (if using GPU-accelerated pandas/numpy)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # STAGE 3: ERA DETECTION (Rust-based)
  # ============================================
  
  era_detector:
    build:
      context: ./feature_extraction/era_detection_rust
      dockerfile: dockerfile
      args:
        CACHEBUST: ${CACHEBUST:-2}
    container_name: era_detection
    tty: true
    stdin_open: true
    command: [
      "--input-parquet", "/app/data/shared_processed/MegaEra_All_Data_processed_segment_1.parquet",
      "--output-dir", "/app/data/shared_processed/era_labels",
      "--output-suffix", "_eras",
      "--db-dsn", "postgresql://postgres:postgres@db:5432/postgres",
      "--signal-cols", "dli_sum,co2_status,light_intensity_lux,co2_measured_ppm,co2_required_ppm,radiation_w_m2,sun_radiation_forecast_w_m2,curtain_2_percent,vent_pos_1_percent,curtain_1_percent,vent_pos_2_percent",
      "--resample-every", "5m",
      "--pelt-min-size", "48",
      "--bocpd-lambda", "200.0",
      "--hmm-states", "5"
    ]
    environment:
      RUST_LOG: debug
      RUST_BACKTRACE: full
      DB_DSN: postgresql://postgres:postgres@db:5432/postgres
    volumes:
      - ./feature_extraction/data/processed:/app/data/shared_processed:rw
    depends_on:
      db:
        condition: service_healthy
      preprocessing:
        condition: service_completed_successfully
    networks:
      - pipeline-net

  # ============================================
  # STAGE 4: FEATURE EXTRACTION (GPU-accelerated)
  # ============================================
  
  feature_extraction:
    build:
      context: ./feature_extraction
      dockerfile: feature/feature.dockerfile
    container_name: feature_extraction
    environment:
      PYTHONUNBUFFERED: "1"
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      FEATURES_TABLE: ${FEATURES_TABLE:-tsfresh_features}
      USE_GPU: ${USE_GPU:-true}
      FEATURE_SET: ${FEATURE_SET:-efficient}
      BATCH_SIZE: ${FEATURE_BATCH_SIZE:-100}
      ERA_LEVEL: ${ERA_LEVEL:-B}
      MIN_ERA_ROWS: ${MIN_ERA_ROWS:-100}
      N_JOBS: ${N_JOBS:--1}
    volumes:
      - ./feature_extraction/data/features:/app/data/output_features:rw
    command: python -m feature.extract_features_enhanced
    depends_on:
      db:
        condition: service_healthy
      # era_detector:
      #   condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for feature extraction
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # STAGE 5: MODEL BUILDING (Multiple objectives)
  # ============================================
  
  # Train all MOEA objective models
  model_builder:
    build:
      context: ./model_builder
      dockerfile: dockerfile
    container_name: model_builder_all_objectives
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
      GPU_TAG: ${GPU_TAG:-default_gpu}
    volumes:
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: ["python", "-m", "src.training.train_all_objectives"]
    depends_on:
      db:
        condition: service_healthy
      # feature_extraction:
      #   condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for model training
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # STAGE 6: MOEA OPTIMIZATION (CPU/GPU)
  # ============================================
  
  moea_optimizer_cpu:
    build:
      context: ./moea_optimizer
      dockerfile: Dockerfile
    container_name: moea_optimizer_cpu
    environment:
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
      CONFIG_PATH: /app/config/moea_config_cpu.toml
      DEVICE: cpu
    volumes:
      - ./moea_optimizer/config:/app/config:ro
      - ./model_builder/models:/app/models:ro
      - ./moea_optimizer/results:/app/results:rw
    command: ["python", "-m", "src.cli", "run", "--config", "/app/config/moea_config_cpu.toml"]
    depends_on:
      db:
        condition: service_healthy
      # model_builder:
      #   condition: service_completed_successfully
    networks:
      - pipeline-net

  moea_optimizer_gpu:
    build:
      context: ./moea_optimizer
      dockerfile: Dockerfile.gpu
    container_name: moea_optimizer_gpu
    # IPC mode for shared memory
    ipc: host
    # Ulimits for memory and stack
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    # Shared memory size (optional, but good to set explicitly)
    # With 32GB system RAM, 8GB shared memory is reasonable
    shm_size: '8gb'
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
      CONFIG_PATH: /app/config/moea_config_gpu.toml
      DEVICE: cuda
      CUDA_DEVICE_ID: 0
    volumes:
      - ./moea_optimizer/config:/app/config:ro
      - ./model_builder/models:/app/models:ro
      - ./moea_optimizer/results:/app/results:rw
    command: ["python", "-m", "src.cli", "run", "--config", "/app/config/moea_config_gpu.toml"]
    depends_on:
      db:
        condition: service_healthy
      # model_builder:
      #   condition: service_completed_successfully
    networks:
      - pipeline-net
    # GPU support for MOEA optimization
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # UTILITY: Run specific pipeline stages
  # ============================================
  
  # Alternative: Train only LSTM surrogate model
  model_builder_lstm:
    build:
      context: ./model_builder
      dockerfile: dockerfile
    container_name: model_builder_lstm
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      GPU_TAG: ${GPU_TAG:-default_gpu}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
    volumes:
      - ./simulation_data_prep/output:/data:ro
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: ["python", "-m", "src.training.train_surrogate", "--epochs", "50", "--data-dir", "/data", "--model-dir", "/models"]
    depends_on:
      db:
        condition: service_healthy
    networks:
      - pipeline-net
    profiles: ["lstm"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Alternative: Train only LightGBM model for single objective
  model_builder_lightgbm:
    build:
      context: ./model_builder
      dockerfile: dockerfile
    container_name: model_builder_lightgbm
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      DB_USER: ${DB_USER:-postgres}
      DB_PASSWORD: ${DB_PASSWORD:-postgres}
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_NAME: ${DB_NAME:-postgres}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-}
    volumes:
      - ./model_builder/models:/models
      - ./model_builder/model_builder_mlflow_staging:/mlflow
    command: ["python", "-m", "src.training.train_lightgbm_surrogate", "--target", "energy_consumption", "--gpu"]
    depends_on:
      db:
        condition: service_healthy
      feature_extraction:
        condition: service_completed_successfully
    networks:
      - pipeline-net
    profiles: ["lightgbm"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# ============================================
# VOLUMES
# ============================================
volumes:
  postgres-data:
  redis-data:
  pgadmin-data:
  mlflow_data:

# ============================================
# NETWORKS
# ============================================
networks:
  pipeline-net:
    name: greenhouse-pipeline
    driver: bridge